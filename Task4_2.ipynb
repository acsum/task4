{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "from tables import *\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.regularizers import l2\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x119</th>\n",
       "      <th>x120</th>\n",
       "      <th>x121</th>\n",
       "      <th>x122</th>\n",
       "      <th>x123</th>\n",
       "      <th>x124</th>\n",
       "      <th>x125</th>\n",
       "      <th>x126</th>\n",
       "      <th>x127</th>\n",
       "      <th>x128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.437475</td>\n",
       "      <td>0.617803</td>\n",
       "      <td>0.781499</td>\n",
       "      <td>0.404499</td>\n",
       "      <td>0.718066</td>\n",
       "      <td>0.554732</td>\n",
       "      <td>0.459540</td>\n",
       "      <td>0.579324</td>\n",
       "      <td>0.648441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523730</td>\n",
       "      <td>0.629020</td>\n",
       "      <td>0.494569</td>\n",
       "      <td>0.563879</td>\n",
       "      <td>0.604331</td>\n",
       "      <td>0.497247</td>\n",
       "      <td>0.335207</td>\n",
       "      <td>0.537755</td>\n",
       "      <td>0.555317</td>\n",
       "      <td>0.458929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.436973</td>\n",
       "      <td>0.611919</td>\n",
       "      <td>0.787908</td>\n",
       "      <td>0.403480</td>\n",
       "      <td>0.725049</td>\n",
       "      <td>0.559196</td>\n",
       "      <td>0.460639</td>\n",
       "      <td>0.588320</td>\n",
       "      <td>0.657027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527735</td>\n",
       "      <td>0.636653</td>\n",
       "      <td>0.486114</td>\n",
       "      <td>0.555791</td>\n",
       "      <td>0.612709</td>\n",
       "      <td>0.491065</td>\n",
       "      <td>0.328115</td>\n",
       "      <td>0.536971</td>\n",
       "      <td>0.551187</td>\n",
       "      <td>0.473370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.437700</td>\n",
       "      <td>0.619443</td>\n",
       "      <td>0.783668</td>\n",
       "      <td>0.405216</td>\n",
       "      <td>0.719925</td>\n",
       "      <td>0.556307</td>\n",
       "      <td>0.461546</td>\n",
       "      <td>0.582350</td>\n",
       "      <td>0.650066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519743</td>\n",
       "      <td>0.628397</td>\n",
       "      <td>0.498987</td>\n",
       "      <td>0.560861</td>\n",
       "      <td>0.607409</td>\n",
       "      <td>0.496805</td>\n",
       "      <td>0.334710</td>\n",
       "      <td>0.533678</td>\n",
       "      <td>0.555739</td>\n",
       "      <td>0.455152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.436640</td>\n",
       "      <td>0.621330</td>\n",
       "      <td>0.783778</td>\n",
       "      <td>0.407233</td>\n",
       "      <td>0.721089</td>\n",
       "      <td>0.554641</td>\n",
       "      <td>0.462114</td>\n",
       "      <td>0.590494</td>\n",
       "      <td>0.658855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518988</td>\n",
       "      <td>0.639706</td>\n",
       "      <td>0.489974</td>\n",
       "      <td>0.562801</td>\n",
       "      <td>0.610364</td>\n",
       "      <td>0.496686</td>\n",
       "      <td>0.326092</td>\n",
       "      <td>0.537641</td>\n",
       "      <td>0.563333</td>\n",
       "      <td>0.465957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0.438236</td>\n",
       "      <td>0.607418</td>\n",
       "      <td>0.781259</td>\n",
       "      <td>0.411375</td>\n",
       "      <td>0.721206</td>\n",
       "      <td>0.550983</td>\n",
       "      <td>0.457552</td>\n",
       "      <td>0.585201</td>\n",
       "      <td>0.653511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528876</td>\n",
       "      <td>0.636095</td>\n",
       "      <td>0.487913</td>\n",
       "      <td>0.566749</td>\n",
       "      <td>0.599150</td>\n",
       "      <td>0.495933</td>\n",
       "      <td>0.325875</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>0.560090</td>\n",
       "      <td>0.466333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   y        x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0  1  0.437475  0.617803  0.781499  0.404499  0.718066  0.554732  0.459540   \n",
       "1  0  0.436973  0.611919  0.787908  0.403480  0.725049  0.559196  0.460639   \n",
       "2  1  0.437700  0.619443  0.783668  0.405216  0.719925  0.556307  0.461546   \n",
       "3  7  0.436640  0.621330  0.783778  0.407233  0.721089  0.554641  0.462114   \n",
       "4  8  0.438236  0.607418  0.781259  0.411375  0.721206  0.550983  0.457552   \n",
       "\n",
       "         x8        x9    ...         x119      x120      x121      x122  \\\n",
       "0  0.579324  0.648441    ...     0.523730  0.629020  0.494569  0.563879   \n",
       "1  0.588320  0.657027    ...     0.527735  0.636653  0.486114  0.555791   \n",
       "2  0.582350  0.650066    ...     0.519743  0.628397  0.498987  0.560861   \n",
       "3  0.590494  0.658855    ...     0.518988  0.639706  0.489974  0.562801   \n",
       "4  0.585201  0.653511    ...     0.528876  0.636095  0.487913  0.566749   \n",
       "\n",
       "       x123      x124      x125      x126      x127      x128  \n",
       "0  0.604331  0.497247  0.335207  0.537755  0.555317  0.458929  \n",
       "1  0.612709  0.491065  0.328115  0.536971  0.551187  0.473370  \n",
       "2  0.607409  0.496805  0.334710  0.533678  0.555739  0.455152  \n",
       "3  0.610364  0.496686  0.326092  0.537641  0.563333  0.465957  \n",
       "4  0.599150  0.495933  0.325875  0.528958  0.560090  0.466333  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labeled = pd.read_hdf(\"train_labeled.h5\", \"train\")\n",
    "train_unlabeled = pd.read_hdf(\"train_unlabeled.h5\", \"train\")\n",
    "test = pd.read_hdf(\"test.h5\", \"test\")\n",
    "\n",
    "#train_unlabeled.shape\n",
    "train_labeled.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_labeled['y']\n",
    "x_train = np.array(train_labeled._drop_axis(['y'], axis=1))\n",
    "#x_train_unlabeled = np.array(train_unlabeled)\n",
    "x_test = np.array(test)\n",
    "\n",
    "#Convert to 1-hot for the model\n",
    "#y_train_hot = np.zeros((y_train.shape[0], 10))\n",
    "#y_train_hot[np.arange(y_train.shape[0]), y_train] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply mean normalization, much faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00013782128309358654"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = len(x_train[1])\n",
    "total = 0\n",
    "\n",
    "#standardize data, so for each x1..x100, do (x-mean)/sd(x) s.t. mean(x) ~= 0\n",
    "for i in range(num_features):\n",
    "    x_train[:,i] = (x_train[:,i] - np.mean(x_train[:,i])) / np.std(x_train[:,i])\n",
    "    x_test[:,i] = (x_test[:,i] - np.mean(x_test[:,i])) / np.std(x_test[:,i])\n",
    "    total += np.mean(x_train[:,i])\n",
    "\n",
    "\n",
    "#adding all the means, we still have a very small # close to 0\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model\n",
    "model = Sequential([Dense(70, input_shape=(128,)), #, W_regularizer=l2(.01)\n",
    "                   Activation('relu'),\n",
    "                   Dense(50),\n",
    "                   Activation('relu'),\n",
    "                   Dense(30),\n",
    "                   Activation('relu'),\n",
    "                   Dense(10),\n",
    "                   Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "9000/9000 [==============================] - 1s 62us/step - loss: 0.9636 - acc: 0.6893\n",
      "Epoch 2/70\n",
      "9000/9000 [==============================] - 1s 71us/step - loss: 0.4250 - acc: 0.8648\n",
      "Epoch 3/70\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 0.3327 - acc: 0.8948\n",
      "Epoch 4/70\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.2742 - acc: 0.9100\n",
      "Epoch 5/70\n",
      "9000/9000 [==============================] - 1s 70us/step - loss: 0.2407 - acc: 0.9214\n",
      "Epoch 6/70\n",
      "9000/9000 [==============================] - 1s 71us/step - loss: 0.2113 - acc: 0.9316\n",
      "Epoch 7/70\n",
      "9000/9000 [==============================] - 1s 76us/step - loss: 0.1832 - acc: 0.9404\n",
      "Epoch 8/70\n",
      "9000/9000 [==============================] - 1s 76us/step - loss: 0.1684 - acc: 0.9461\n",
      "Epoch 9/70\n",
      "9000/9000 [==============================] - 1s 73us/step - loss: 0.1464 - acc: 0.9507\n",
      "Epoch 10/70\n",
      "9000/9000 [==============================] - 1s 76us/step - loss: 0.1330 - acc: 0.9567\n",
      "Epoch 11/70\n",
      "9000/9000 [==============================] - 1s 78us/step - loss: 0.1129 - acc: 0.9623\n",
      "Epoch 12/70\n",
      "9000/9000 [==============================] - 1s 77us/step - loss: 0.1041 - acc: 0.9661: 0s - loss: 0.0993 -\n",
      "Epoch 13/70\n",
      "9000/9000 [==============================] - 1s 70us/step - loss: 0.0908 - acc: 0.9699\n",
      "Epoch 14/70\n",
      "9000/9000 [==============================] - 1s 72us/step - loss: 0.0824 - acc: 0.9730\n",
      "Epoch 15/70\n",
      "9000/9000 [==============================] - 1s 72us/step - loss: 0.0757 - acc: 0.9756\n",
      "Epoch 16/70\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 0.0663 - acc: 0.9786\n",
      "Epoch 17/70\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0600 - acc: 0.9806\n",
      "Epoch 18/70\n",
      "9000/9000 [==============================] - 1s 72us/step - loss: 0.0505 - acc: 0.9859\n",
      "Epoch 19/70\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0584 - acc: 0.9789\n",
      "Epoch 20/70\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0514 - acc: 0.9824\n",
      "Epoch 21/70\n",
      "9000/9000 [==============================] - 1s 76us/step - loss: 0.0430 - acc: 0.9857\n",
      "Epoch 22/70\n",
      "9000/9000 [==============================] - 1s 71us/step - loss: 0.0327 - acc: 0.9896\n",
      "Epoch 23/70\n",
      "9000/9000 [==============================] - 1s 72us/step - loss: 0.0337 - acc: 0.9903\n",
      "Epoch 24/70\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 0.0324 - acc: 0.9894\n",
      "Epoch 25/70\n",
      "9000/9000 [==============================] - 1s 74us/step - loss: 0.0322 - acc: 0.9907\n",
      "Epoch 26/70\n",
      "9000/9000 [==============================] - 1s 73us/step - loss: 0.0420 - acc: 0.9858\n",
      "Epoch 27/70\n",
      "9000/9000 [==============================] - 1s 74us/step - loss: 0.0323 - acc: 0.9896\n",
      "Epoch 28/70\n",
      "9000/9000 [==============================] - 1s 75us/step - loss: 0.0316 - acc: 0.9898\n",
      "Epoch 29/70\n",
      "9000/9000 [==============================] - 1s 73us/step - loss: 0.0199 - acc: 0.9948\n",
      "Epoch 30/70\n",
      "9000/9000 [==============================] - 1s 71us/step - loss: 0.0252 - acc: 0.9927\n",
      "Epoch 31/70\n",
      "9000/9000 [==============================] - 1s 76us/step - loss: 0.0240 - acc: 0.9927: 0s - loss: 0.0220 - acc: 0 - ETA: 0s - loss: 0.0235 - acc: 0.993 - ETA: 0s - loss: 0.0234 - acc: 0.992\n",
      "Epoch 32/70\n",
      "9000/9000 [==============================] - 1s 74us/step - loss: 0.0220 - acc: 0.9924\n",
      "Epoch 33/70\n",
      "9000/9000 [==============================] - 1s 75us/step - loss: 0.0317 - acc: 0.9901\n",
      "Epoch 34/70\n",
      "9000/9000 [==============================] - 1s 72us/step - loss: 0.0323 - acc: 0.9897: 0s - loss: 0.0304 - acc: 0.\n",
      "Epoch 35/70\n",
      "9000/9000 [==============================] - 1s 82us/step - loss: 0.0309 - acc: 0.9900: 1s - loss: 0.0213 - acc: 0.99 - ETA: 0s - loss: 0.0255 - acc: 0.991 - ETA: 0s - loss: 0.0294 - \n",
      "Epoch 36/70\n",
      "9000/9000 [==============================] - 1s 71us/step - loss: 0.0396 - acc: 0.9862\n",
      "Epoch 37/70\n",
      "9000/9000 [==============================] - 1s 85us/step - loss: 0.0185 - acc: 0.9930\n",
      "Epoch 38/70\n",
      "9000/9000 [==============================] - 1s 76us/step - loss: 0.0114 - acc: 0.9963\n",
      "Epoch 39/70\n",
      "9000/9000 [==============================] - 1s 72us/step - loss: 0.0090 - acc: 0.9974\n",
      "Epoch 40/70\n",
      "9000/9000 [==============================] - 1s 70us/step - loss: 0.0314 - acc: 0.9898: 0s - loss: 0.0229 - acc:\n",
      "Epoch 41/70\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0424 - acc: 0.9859\n",
      "Epoch 42/70\n",
      "9000/9000 [==============================] - 1s 71us/step - loss: 0.0097 - acc: 0.9973\n",
      "Epoch 43/70\n",
      "9000/9000 [==============================] - 1s 75us/step - loss: 0.0060 - acc: 0.9983: 0s - loss: 0.0044 - acc\n",
      "Epoch 44/70\n",
      "9000/9000 [==============================] - 1s 70us/step - loss: 0.0033 - acc: 0.9991\n",
      "Epoch 45/70\n",
      "9000/9000 [==============================] - 1s 74us/step - loss: 0.0090 - acc: 0.9976\n",
      "Epoch 46/70\n",
      "9000/9000 [==============================] - 1s 84us/step - loss: 0.0727 - acc: 0.9781\n",
      "Epoch 47/70\n",
      "9000/9000 [==============================] - 1s 81us/step - loss: 0.0531 - acc: 0.9821\n",
      "Epoch 48/70\n",
      "9000/9000 [==============================] - 1s 76us/step - loss: 0.0150 - acc: 0.9957\n",
      "Epoch 49/70\n",
      "9000/9000 [==============================] - 1s 71us/step - loss: 0.0072 - acc: 0.9982\n",
      "Epoch 50/70\n",
      "9000/9000 [==============================] - 1s 73us/step - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 51/70\n",
      "9000/9000 [==============================] - 1s 77us/step - loss: 0.0023 - acc: 0.9996\n",
      "Epoch 52/70\n",
      "9000/9000 [==============================] - 1s 71us/step - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 53/70\n",
      "9000/9000 [==============================] - 1s 70us/step - loss: 0.0112 - acc: 0.9970\n",
      "Epoch 54/70\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 0.0592 - acc: 0.9801\n",
      "Epoch 55/70\n",
      "9000/9000 [==============================] - 1s 68us/step - loss: 0.0452 - acc: 0.9859\n",
      "Epoch 56/70\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 0.0301 - acc: 0.9897\n",
      "Epoch 57/70\n",
      "9000/9000 [==============================] - 1s 74us/step - loss: 0.0123 - acc: 0.9963\n",
      "Epoch 58/70\n",
      "9000/9000 [==============================] - 1s 74us/step - loss: 0.0058 - acc: 0.9986\n",
      "Epoch 59/70\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.0018 - acc: 0.9998\n",
      "Epoch 60/70\n",
      "9000/9000 [==============================] - 1s 75us/step - loss: 7.1907e-04 - acc: 1.0000\n",
      "Epoch 61/70\n",
      "9000/9000 [==============================] - 1s 73us/step - loss: 8.9928e-04 - acc: 0.9998\n",
      "Epoch 62/70\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 0.0036 - acc: 0.9988\n",
      "Epoch 63/70\n",
      "9000/9000 [==============================] - 1s 77us/step - loss: 0.1003 - acc: 0.9700\n",
      "Epoch 64/70\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.0385 - acc: 0.9862\n",
      "Epoch 65/70\n",
      "9000/9000 [==============================] - 1s 74us/step - loss: 0.0149 - acc: 0.9958\n",
      "Epoch 66/70\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 0.0030 - acc: 0.9996: 0s - loss: 0.0066 - a\n",
      "Epoch 67/70\n",
      "9000/9000 [==============================] - 1s 73us/step - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 68/70\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 69/70\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 0.0021 - acc: 0.9996\n",
      "Epoch 70/70\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.0019 - acc: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2948c3fea20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training\n",
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "#Easy way to convert to one-hot\n",
    "y_train_hot2 = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "\n",
    "model.fit(x_train, y_train_hot2, epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_one_hot = model.predict(x_test)\n",
    "\n",
    "#Convert predictions back from 1-hot\n",
    "pred = []\n",
    "\n",
    "for line in pred_one_hot:\n",
    "    pred.append(np.argmax(line))\n",
    "    \n",
    "d = {'Id': test.index, 'y': pred}\n",
    "out = pd.DataFrame(d)\n",
    "out.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
